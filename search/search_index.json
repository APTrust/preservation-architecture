{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"APTrust Preservation Services Architecture This site describes the architectural components and general workflow of APTrust's third-generation preservation services. As of August, 2020, APTrust's production and demo repositories are running on our second-generation services suite, code-named Exchange. The services described in these documents are currently being tested in our staging environment. In replacing our existing, stable, and heavily patched second-generation services, the goals of the new service suite include: Modularize code to separate concerns Improve code clarity and maintainability Improve test coverage Improve horizontal scalability by storing interim processing information in network accessible services instead of on local disk Support BagIt profiles other than APTrust Support S3-compliant storage providers other than AWS Eliminate reliance on expensive and problematic EBS volumes The new preservation architecture also supports the primary goals of the prior architecture, which include: Resilience - The system must be able to recover from failures and resume whatever processing was occuring at the time of failure without any loss of information. Reliability - The system must process all valid requests for ingest, deletion, fixity checking, restoration to completion. Accountability - The system must maintain a record of all actions performed on all objects and files. It must also record the existence and outcome of all tasks that were requested but could not be completed. Security - All files in preservation storage must be unreachable except through the administrative interface of Pharos. Destructive actions, such as deletions, require multiple approvals from depositors. Transparency - Depositors must be able to see the present state of any item, ingested or not, that is within the normal flow of processing. Administrators must be able to see all that and information about abnormal or failed states. Service Provider Agnosticism - APTrust services must, whenever possible, avoid relying on vendor-specific technologies. The entire system should be portable to any cloud provider or to any data center that runs Linux servers. Preservation services adhere to this principle by being open source, written in a cross-platform language (Go), and relying only on open source services such as Redis and NSQ, and standardized or de facto protocols and APIs such as HTTPS and S3. Performance takes last place in our priority list. In general, it doesn't matter to a depositor whether it takes ten minutes or ten hours for us to ingest materials. However, APTrust receives periodic ingest floods which our second-generation architecture could take weeks to clear and which required regular human intervention. The horizontal scaling capabilities of the new architecture should allow us to process weeks-long backlogs in a matter of days, with little or no human intervention. Services Provided APTrust provides the following preservation services: Ingest Ongoing Fixity Checks Deletion Restoration Each service is composed of the following: Temporary Storage includes S3 buckets in Amazon Web Services (AWS) to which depositors can upload items for ingest (receiving buckets), from which depositors can download restored objects and files (restoration buckets), and in which APTrust ingest processes store temporary data (staging buckets). Preservation Storage consists of Glacier and S3 buckets provided by AWS and other commercial cloud providers. Depositors have no access to these buckets, and even APTrust staff have limited access. Pharos , a Rails application backed by a Postgres database, which tracks information about all objects and files preserved in (and deleted from) preservation storage. Pharos also allows authorized users to requestion deletion and restoration of objects and files. Microservices running on Amazon EC2 instances perform the work required for ingest, fixity checking, deletion and restoration. Supporting Services help microservices orchestrate multistep processes. These include NSQ, for managing work queues, and Redis for tracking internal state and metadata across processes in which multiple microservices must coordinate work.","title":"Home"},{"location":"#aptrust-preservation-services-architecture","text":"This site describes the architectural components and general workflow of APTrust's third-generation preservation services. As of August, 2020, APTrust's production and demo repositories are running on our second-generation services suite, code-named Exchange. The services described in these documents are currently being tested in our staging environment. In replacing our existing, stable, and heavily patched second-generation services, the goals of the new service suite include: Modularize code to separate concerns Improve code clarity and maintainability Improve test coverage Improve horizontal scalability by storing interim processing information in network accessible services instead of on local disk Support BagIt profiles other than APTrust Support S3-compliant storage providers other than AWS Eliminate reliance on expensive and problematic EBS volumes The new preservation architecture also supports the primary goals of the prior architecture, which include: Resilience - The system must be able to recover from failures and resume whatever processing was occuring at the time of failure without any loss of information. Reliability - The system must process all valid requests for ingest, deletion, fixity checking, restoration to completion. Accountability - The system must maintain a record of all actions performed on all objects and files. It must also record the existence and outcome of all tasks that were requested but could not be completed. Security - All files in preservation storage must be unreachable except through the administrative interface of Pharos. Destructive actions, such as deletions, require multiple approvals from depositors. Transparency - Depositors must be able to see the present state of any item, ingested or not, that is within the normal flow of processing. Administrators must be able to see all that and information about abnormal or failed states. Service Provider Agnosticism - APTrust services must, whenever possible, avoid relying on vendor-specific technologies. The entire system should be portable to any cloud provider or to any data center that runs Linux servers. Preservation services adhere to this principle by being open source, written in a cross-platform language (Go), and relying only on open source services such as Redis and NSQ, and standardized or de facto protocols and APIs such as HTTPS and S3. Performance takes last place in our priority list. In general, it doesn't matter to a depositor whether it takes ten minutes or ten hours for us to ingest materials. However, APTrust receives periodic ingest floods which our second-generation architecture could take weeks to clear and which required regular human intervention. The horizontal scaling capabilities of the new architecture should allow us to process weeks-long backlogs in a matter of days, with little or no human intervention.","title":"APTrust Preservation Services Architecture"},{"location":"#services-provided","text":"APTrust provides the following preservation services: Ingest Ongoing Fixity Checks Deletion Restoration Each service is composed of the following: Temporary Storage includes S3 buckets in Amazon Web Services (AWS) to which depositors can upload items for ingest (receiving buckets), from which depositors can download restored objects and files (restoration buckets), and in which APTrust ingest processes store temporary data (staging buckets). Preservation Storage consists of Glacier and S3 buckets provided by AWS and other commercial cloud providers. Depositors have no access to these buckets, and even APTrust staff have limited access. Pharos , a Rails application backed by a Postgres database, which tracks information about all objects and files preserved in (and deleted from) preservation storage. Pharos also allows authorized users to requestion deletion and restoration of objects and files. Microservices running on Amazon EC2 instances perform the work required for ingest, fixity checking, deletion and restoration. Supporting Services help microservices orchestrate multistep processes. These include NSQ, for managing work queues, and Redis for tracking internal state and metadata across processes in which multiple microservices must coordinate work.","title":"Services Provided"},{"location":"components/","text":"Overview","title":"Overview"},{"location":"components/#overview","text":"","title":"Overview"},{"location":"components/microservices/","text":"Microservices","title":"Microservices"},{"location":"components/microservices/#microservices","text":"","title":"Microservices"},{"location":"components/nsq/","text":"NSQ","title":"NSQ"},{"location":"components/nsq/#nsq","text":"","title":"NSQ"},{"location":"components/pharos/","text":"Pharos","title":"Pharos"},{"location":"components/pharos/#pharos","text":"","title":"Pharos"},{"location":"components/preservation/","text":"Preservation","title":"Preservation"},{"location":"components/preservation/#preservation","text":"","title":"Preservation"},{"location":"components/rds/","text":"Postgres RDS Database","title":"Postgres RDS Database"},{"location":"components/rds/#postgres-rds-database","text":"","title":"Postgres RDS Database"},{"location":"components/receiving/","text":"Receiving","title":"Receiving"},{"location":"components/receiving/#receiving","text":"","title":"Receiving"},{"location":"components/redis/","text":"Redis","title":"Redis"},{"location":"components/redis/#redis","text":"","title":"Redis"},{"location":"components/restoration/","text":"Restoration","title":"Restoration"},{"location":"components/restoration/#restoration","text":"","title":"Restoration"},{"location":"scaling/","text":"Overview This page describes considerations for scaling each of our services. All scaling is horizontal, meaning we simply run services on more servers to handle increasing loads. Most of the time, a single server running all services (ingest, fixity, deletion and restoration) is fine. During ingest floods and periods of heavy restoration, we need to scale only specific micro services, as described below. When adding new instances, we must ensure that workers on the instance have network access to the following services: Pharos, for getting and recording WorkItems. NSQ, for knowing which items to work on. Redis, for accessing and preserving interim processing data. Keep in mind that Pharos, Redis, and NSQ may be running on a private subnet or VPN. The new servers must be able to reach that subnet or connect to that VPN. In addition, additional servers to handle temporary load spikes should be in the US East region for fast access to primary preservation storage, which is in S3 in US East. Ingest Extensive profiling has shown us that bandwidth is the bottleneck for ingest operations. The pre-fetch worker , staging uploader and preservation uploader all use substantial bandwidth, and all should be scaled horizontally during periods of heavy ingest. Format identification uses at least a noticeable amount of bandwidth and CPU. Additional servers may be relieve pressure from the primary server during heavy ingest. The validation , storage validation , and cleanup workers use few resources and would generally not benefit from additional instances. The reingest worker may flood Pharos with requests when reingesting bags with many files, and the metadata recording worker taxes Pharos on every ingest. For these reasons, we should not run more than one instance of either of these workers. Instead, we can tune the number of Go routines within a single instance of each worker to ensure they utilize Pharos to its maximum capacity without overloading it. Fixity Checking Because fixity checking is handled by a single worker, we can scale up by starting a new EC2 instance and running an additional fixity checker there. Because fixity checking is bandwidth-intensive, consider choosing an instance type with medium or better network IO. Micro and small instances will be of little help. Deletion Deletion scaling may never be necessary. The only time we've ever had deletion backlogs have been after depositors request bulk deletion of tens of thousands of files. Even then, the depositor doesn't care if it takes ten minutes or 24 hours for the deletion to complete. Restoration TBD. The third-generation restoration code isn't written yet.","title":"Overview"},{"location":"scaling/#overview","text":"This page describes considerations for scaling each of our services. All scaling is horizontal, meaning we simply run services on more servers to handle increasing loads. Most of the time, a single server running all services (ingest, fixity, deletion and restoration) is fine. During ingest floods and periods of heavy restoration, we need to scale only specific micro services, as described below. When adding new instances, we must ensure that workers on the instance have network access to the following services: Pharos, for getting and recording WorkItems. NSQ, for knowing which items to work on. Redis, for accessing and preserving interim processing data. Keep in mind that Pharos, Redis, and NSQ may be running on a private subnet or VPN. The new servers must be able to reach that subnet or connect to that VPN. In addition, additional servers to handle temporary load spikes should be in the US East region for fast access to primary preservation storage, which is in S3 in US East.","title":"Overview"},{"location":"scaling/#ingest","text":"Extensive profiling has shown us that bandwidth is the bottleneck for ingest operations. The pre-fetch worker , staging uploader and preservation uploader all use substantial bandwidth, and all should be scaled horizontally during periods of heavy ingest. Format identification uses at least a noticeable amount of bandwidth and CPU. Additional servers may be relieve pressure from the primary server during heavy ingest. The validation , storage validation , and cleanup workers use few resources and would generally not benefit from additional instances. The reingest worker may flood Pharos with requests when reingesting bags with many files, and the metadata recording worker taxes Pharos on every ingest. For these reasons, we should not run more than one instance of either of these workers. Instead, we can tune the number of Go routines within a single instance of each worker to ensure they utilize Pharos to its maximum capacity without overloading it.","title":"Ingest"},{"location":"scaling/#fixity-checking","text":"Because fixity checking is handled by a single worker, we can scale up by starting a new EC2 instance and running an additional fixity checker there. Because fixity checking is bandwidth-intensive, consider choosing an instance type with medium or better network IO. Micro and small instances will be of little help.","title":"Fixity Checking"},{"location":"scaling/#deletion","text":"Deletion scaling may never be necessary. The only time we've ever had deletion backlogs have been after depositors request bulk deletion of tens of thousands of files. Even then, the depositor doesn't care if it takes ten minutes or 24 hours for the deletion to complete.","title":"Deletion"},{"location":"scaling/#restoration","text":"TBD. The third-generation restoration code isn't written yet.","title":"Restoration"},{"location":"services/deletion/","text":"Deletion Depositors can request deletion of individual files or entire intellectual objects through the Pharos Web UI, but not through the API. This is by design, as we want to prevent large-scale accidental automated deletion. When a depositor requests a deletion, Pharos sends an email to all admin users at the depositing institution saying that the user has requested a deletion and asking the admin to approve. The email names the user who requested the deletion and lists the files or objects the user wants to delete. It also includes a link the admin can click to approve the deletion. No deletions will proceed without approval from the institutional admin. Some smaller institutions may have only one active administrator. In those cases, the person who approves the deletion may be the same person who requested it. APTrust considers it the depositor's duty to ensure they trust this person to manage deletions unchecked, or to add a second approver if they don't. When an admin clicks the link to approve a deletion, Pharos ensures the admin either has an active, valid, authenticated session, or it forces them to log in. This prevents non-authorized users from using the link to approve deletions. The deletion process follows the steps outlined under bulk deletion below, except 1) it is initiated directly by an institutional user, and 2) it does not require the approval of an APTrust administrator. Bulk Deletion Pharos includes an endpoint to initiate bulk deletions. This endpoint is available only to APTrust administrators. The bulk deletion process follows these steps: The depositor sends APTrust a list (usually a spreadsheet) of files and/or objects they want to delete. An APTrust administrator feeds the list to the bulk deletion initiation endpoint in Pharos. Pharos emails all administrators at the depositing institution saying that the initiating user has requested a bulk deletion. The email includes an attachment listing the items to be deleted. The institutional admin clicks a link to approve the bulk deletion. Pharos emails APTrust admins, telling them the institution has approved the bulk deletion. The institutional admin approves the bulk deletion. Pharos creates a deletion WorkItem for each file to be deleted. A cron job that runs every 10 minutes or checks for new deletion requests and copies the WorkItem IDs of those requests to apt_file_delete topic in NSQ. The apt_delete worker checks that all required approvals have been granted, and if so, deletes the item. apt_delete creates a PREMIS event in Pharos describing when the file was deleted, who requested the deletion, and who approved it. apt_delete marks the WorkItem and NSQ message complete.","title":"Deletion"},{"location":"services/deletion/#deletion","text":"Depositors can request deletion of individual files or entire intellectual objects through the Pharos Web UI, but not through the API. This is by design, as we want to prevent large-scale accidental automated deletion. When a depositor requests a deletion, Pharos sends an email to all admin users at the depositing institution saying that the user has requested a deletion and asking the admin to approve. The email names the user who requested the deletion and lists the files or objects the user wants to delete. It also includes a link the admin can click to approve the deletion. No deletions will proceed without approval from the institutional admin. Some smaller institutions may have only one active administrator. In those cases, the person who approves the deletion may be the same person who requested it. APTrust considers it the depositor's duty to ensure they trust this person to manage deletions unchecked, or to add a second approver if they don't. When an admin clicks the link to approve a deletion, Pharos ensures the admin either has an active, valid, authenticated session, or it forces them to log in. This prevents non-authorized users from using the link to approve deletions. The deletion process follows the steps outlined under bulk deletion below, except 1) it is initiated directly by an institutional user, and 2) it does not require the approval of an APTrust administrator.","title":"Deletion"},{"location":"services/deletion/#bulk-deletion","text":"Pharos includes an endpoint to initiate bulk deletions. This endpoint is available only to APTrust administrators. The bulk deletion process follows these steps: The depositor sends APTrust a list (usually a spreadsheet) of files and/or objects they want to delete. An APTrust administrator feeds the list to the bulk deletion initiation endpoint in Pharos. Pharos emails all administrators at the depositing institution saying that the initiating user has requested a bulk deletion. The email includes an attachment listing the items to be deleted. The institutional admin clicks a link to approve the bulk deletion. Pharos emails APTrust admins, telling them the institution has approved the bulk deletion. The institutional admin approves the bulk deletion. Pharos creates a deletion WorkItem for each file to be deleted. A cron job that runs every 10 minutes or checks for new deletion requests and copies the WorkItem IDs of those requests to apt_file_delete topic in NSQ. The apt_delete worker checks that all required approvals have been granted, and if so, deletes the item. apt_delete creates a PREMIS event in Pharos describing when the file was deleted, who requested the deletion, and who approved it. apt_delete marks the WorkItem and NSQ message complete.","title":"Bulk Deletion"},{"location":"services/fixity/","text":"Fixity Checking APTrust checks fixity on all files in S3 preservation storage approximately every 90 days. We do not check fixity on files stored in Glacier because the process of restoring Glacier files is cumbersome and expensive. While our service contract with depositors says we will check fixity on all S3 files every 90 days, this may occasionally slip to 91 or 92 days in practice because during ingest floods, we prioritize ingest over fixity checking. In the second generation of APTrust (Exchange), we were not able to scale services horizontally and sometimes turned off fixity checking for 1-2 days so ingest services could use all of the server's CPU and bandwidth. This should not be an issue with the third-generation code. Because ingest floods tend to occur in regular cycles (before the Spring and Fall meetings, and at the end of the calendar year), fixity check floods tend to coincide with ingest floods. That is, while ingesting a million new files, we may also have to run fixity checks on the million files we ingested 360 days ago. Again, this should be less of a problem in the third-generation system. Fixity Check Process A cron job called apt_queue_fixity runs every 30 minutes on one of our EC2 instances querying Pharos for files that have not had a fixity check in the past 90 days. It sends the identifiers of up to 2500 files on each run into an NSQ topic called apt_fixity_topic . A worker called apt_fixity_check picks up the file identifier from NSQ and retrieves the GenericFile record from Pharos. This record contains, among other things, the URL of the file in preservation storage, and the most current md5 and sha256 digests for the file. The worker retrieves the file from S3 preservation storage, streaming it through Golang's md5 and sha256 hash functions. The reads are fast because the data never touches the disk. It goes from S3 through the hash functions and into /dev/null. The worker compares each fixity value (md5 and sha256) to the known fixity values in the GenericFile record it got from Pharos. The worker records one fixity check PREMIS event in Pharos for each digest. In each case, if the digest matches, it records a successful event. If the digest does not match, it records a failed event, along with the expected and actual checksums. The fixity check process is fast and efficient, often processing over 100,000 files per day with little noticeable effect on CPU or memory. Fixity checking is the only operation on preservation files that does not produce a WorkItem record in Pharos. (Ingests, deletions, and restorations do produce WorkItems.) Fixity checks produce only PREMIS events.","title":"Fixity Checking"},{"location":"services/fixity/#fixity-checking","text":"APTrust checks fixity on all files in S3 preservation storage approximately every 90 days. We do not check fixity on files stored in Glacier because the process of restoring Glacier files is cumbersome and expensive. While our service contract with depositors says we will check fixity on all S3 files every 90 days, this may occasionally slip to 91 or 92 days in practice because during ingest floods, we prioritize ingest over fixity checking. In the second generation of APTrust (Exchange), we were not able to scale services horizontally and sometimes turned off fixity checking for 1-2 days so ingest services could use all of the server's CPU and bandwidth. This should not be an issue with the third-generation code. Because ingest floods tend to occur in regular cycles (before the Spring and Fall meetings, and at the end of the calendar year), fixity check floods tend to coincide with ingest floods. That is, while ingesting a million new files, we may also have to run fixity checks on the million files we ingested 360 days ago. Again, this should be less of a problem in the third-generation system.","title":"Fixity Checking"},{"location":"services/fixity/#fixity-check-process","text":"A cron job called apt_queue_fixity runs every 30 minutes on one of our EC2 instances querying Pharos for files that have not had a fixity check in the past 90 days. It sends the identifiers of up to 2500 files on each run into an NSQ topic called apt_fixity_topic . A worker called apt_fixity_check picks up the file identifier from NSQ and retrieves the GenericFile record from Pharos. This record contains, among other things, the URL of the file in preservation storage, and the most current md5 and sha256 digests for the file. The worker retrieves the file from S3 preservation storage, streaming it through Golang's md5 and sha256 hash functions. The reads are fast because the data never touches the disk. It goes from S3 through the hash functions and into /dev/null. The worker compares each fixity value (md5 and sha256) to the known fixity values in the GenericFile record it got from Pharos. The worker records one fixity check PREMIS event in Pharos for each digest. In each case, if the digest matches, it records a successful event. If the digest does not match, it records a failed event, along with the expected and actual checksums. The fixity check process is fast and efficient, often processing over 100,000 files per day with little noticeable effect on CPU or memory. Fixity checking is the only operation on preservation files that does not produce a WorkItem record in Pharos. (Ingests, deletions, and restorations do produce WorkItems.) Fixity checks produce only PREMIS events.","title":"Fixity Check Process"},{"location":"services/restoration/","text":"Restoration Depositors can request restoration of files or objects through the Pharos Web UI. Clicking the Restore button for a file restores a single file; for an object, it restores all files belonging to that object in BagIt format. When the user clicks Restore : Pharos creates a WorkItem saying that user X has requested restoration of file or object Y. The apt_queue cron job, which runs every 10 minutes or so, copies the WorkItem ID of the restoration request into the appropriate topic in NSQ. NSQ passes the WorkItem ID to a worker to fulfill the request. Restoring Files The file restoration worker performs the following tasks: After receiving the WorkItem ID from NSQ, it retrieves the WorkItem from Pharos. The worker retrieves the GenericFile record from Pharos of the file to be restored. The worker copies the file directly from preservation storage to the depositor's receiving bucket. The worker marks the WorkItem complete, and includes in the WorkItem note the URL from which the depositor may retrieve the file. The worker marks the NSQ message as finished. Restoring Objects Object restoration is more complex, since restoring an object requires retrieving and packaging multiple files before sending the entire restored package to the depositor's restoration bucket. The object restoration worker performs the following tasks: After receiving the WorkItem ID from NSQ, it retrieves the WorkItem from Pharos. The worker retrieves the IntellectualObject record from Pharos of the object to be restored. The worker retrieves each GenericFile record belonging to that object from Pharos. For each active GenericFile (where state = \"A\" and not \"D\"), the worker copies the file from preservation storage to a staging area for bagging. Note that, other than bagit.txt, APTrust preserves tag files in addition to payload files. The worker validates the actual checksums of each file during the download to staging. If any file cannot be found, or if any file's checksum does not match the most recent checksum in Pharos, the restoration fails. The worker bags all of the files (including perserved tag files) according to the profile used when the bag was initially ingested. The profile information is part of the IntellectualObject record, and may be APTrust or Beyond the Repository (BTR). As part of the bagging process, the restoration worker creates manifests and tag manifests. The worker requests a list from Pharos of all PREMIS events pertaining to this object. It includes this list in the bag as a JSON file. The primary reason for this is so depositors can see that some items in the bag may have been deleted or re-ingested while APTrust had custody of the object. The worker validates the bag. The worker copies the bag to the depositor's receiving bucket. The worker marks the WorkItem complete, and includes in the WorkItem note the URL from which the depositor may retrieve the file. The worker marks the NSQ message as finished. Note that because APTrust rebags objects when it restores them, the restored bag will not be byte-for-byte identical to the initially ingested bag. We do guarantee that the payload will be complete, and that the new bag will include the tag files from the original (except for bagit.txt, which we reconstitute), the restored bag can have the following differences: Items in the manifest may be listed in a different order than the original. The restored bag may include additional manifests or tag manifests not present in the original bag. The restored bag will not include files that were deleted during the object's time in APTrust's custody. Restoring from Glacier Glacier restorations follow the steps outlined above for files and objects, with these additional steps at the beginning: The WorkItem IDs of Glacier items first go into an NSQ topic called apt_glacier_restore_init . The apt_glacier_restore_init worker issues one Glacier restore request per file to the vault holding the files. The worker checks the status of the restore requests every few hours. When all Glacier files have been copied to S3, the Glacier restore worker pushes the WorkItem ID into the normal NSQ restoration topic. From there, the restoration proceeds as described above.","title":"Restoration"},{"location":"services/restoration/#restoration","text":"Depositors can request restoration of files or objects through the Pharos Web UI. Clicking the Restore button for a file restores a single file; for an object, it restores all files belonging to that object in BagIt format. When the user clicks Restore : Pharos creates a WorkItem saying that user X has requested restoration of file or object Y. The apt_queue cron job, which runs every 10 minutes or so, copies the WorkItem ID of the restoration request into the appropriate topic in NSQ. NSQ passes the WorkItem ID to a worker to fulfill the request.","title":"Restoration"},{"location":"services/restoration/#restoring-files","text":"The file restoration worker performs the following tasks: After receiving the WorkItem ID from NSQ, it retrieves the WorkItem from Pharos. The worker retrieves the GenericFile record from Pharos of the file to be restored. The worker copies the file directly from preservation storage to the depositor's receiving bucket. The worker marks the WorkItem complete, and includes in the WorkItem note the URL from which the depositor may retrieve the file. The worker marks the NSQ message as finished.","title":"Restoring Files"},{"location":"services/restoration/#restoring-objects","text":"Object restoration is more complex, since restoring an object requires retrieving and packaging multiple files before sending the entire restored package to the depositor's restoration bucket. The object restoration worker performs the following tasks: After receiving the WorkItem ID from NSQ, it retrieves the WorkItem from Pharos. The worker retrieves the IntellectualObject record from Pharos of the object to be restored. The worker retrieves each GenericFile record belonging to that object from Pharos. For each active GenericFile (where state = \"A\" and not \"D\"), the worker copies the file from preservation storage to a staging area for bagging. Note that, other than bagit.txt, APTrust preserves tag files in addition to payload files. The worker validates the actual checksums of each file during the download to staging. If any file cannot be found, or if any file's checksum does not match the most recent checksum in Pharos, the restoration fails. The worker bags all of the files (including perserved tag files) according to the profile used when the bag was initially ingested. The profile information is part of the IntellectualObject record, and may be APTrust or Beyond the Repository (BTR). As part of the bagging process, the restoration worker creates manifests and tag manifests. The worker requests a list from Pharos of all PREMIS events pertaining to this object. It includes this list in the bag as a JSON file. The primary reason for this is so depositors can see that some items in the bag may have been deleted or re-ingested while APTrust had custody of the object. The worker validates the bag. The worker copies the bag to the depositor's receiving bucket. The worker marks the WorkItem complete, and includes in the WorkItem note the URL from which the depositor may retrieve the file. The worker marks the NSQ message as finished. Note that because APTrust rebags objects when it restores them, the restored bag will not be byte-for-byte identical to the initially ingested bag. We do guarantee that the payload will be complete, and that the new bag will include the tag files from the original (except for bagit.txt, which we reconstitute), the restored bag can have the following differences: Items in the manifest may be listed in a different order than the original. The restored bag may include additional manifests or tag manifests not present in the original bag. The restored bag will not include files that were deleted during the object's time in APTrust's custody.","title":"Restoring Objects"},{"location":"services/restoration/#restoring-from-glacier","text":"Glacier restorations follow the steps outlined above for files and objects, with these additional steps at the beginning: The WorkItem IDs of Glacier items first go into an NSQ topic called apt_glacier_restore_init . The apt_glacier_restore_init worker issues one Glacier restore request per file to the vault holding the files. The worker checks the status of the restore requests every few hours. When all Glacier files have been copied to S3, the Glacier restore worker pushes the WorkItem ID into the normal NSQ restoration topic. From there, the restoration proceeds as described above.","title":"Restoring from Glacier"},{"location":"services/ingest/","text":"Overview To do: diagram to show components and flow. To do: simple bullet points outlining flow and briefly describing the purpose of each step.","title":"Overview"},{"location":"services/ingest/#overview","text":"To do: diagram to show components and flow. To do: simple bullet points outlining flow and briefly describing the purpose of each step.","title":"Overview"},{"location":"services/ingest/bucket_reader/","text":"Bucket Reader The ingest_bucket_reader is a cron job that scans receiving buckets for newly uploaded items. For each item, it checks Pharos to see if an ingest WorkItem with matching bucket, key, and etag exists. If not, the bucket reader creates it and adds the new WorkItem ID to NSQ's ingest pre-fetch topic. While we could have set up an S3 trigger on the receiving buckets to do this work, we chose not to for a few reasons. First, during our scheduled maintenance windows, Pharos can be offline for two hours or more. In addition, during routine deployments, Pharos may be unavailable for a few seconds. In both cases, data from the S3 triggers would be lost, as there would be no endpoint to receive them. Second, to ensure our system is portable and not dependent on vendor-specific features, we wanted to avoid AWS-specific lambda triggers that may not port to open-source S3-compliant systems like Minio or even to other cloud providers like Wasabi. Third, the cron job provides a simple mechanism for throttling ingest during periods of heavy load. It's easier to change the timing on a single cron job than to suspend and then re-enable triggers in twenty or so S3 buckets.","title":"Bucket Reader"},{"location":"services/ingest/bucket_reader/#bucket-reader","text":"The ingest_bucket_reader is a cron job that scans receiving buckets for newly uploaded items. For each item, it checks Pharos to see if an ingest WorkItem with matching bucket, key, and etag exists. If not, the bucket reader creates it and adds the new WorkItem ID to NSQ's ingest pre-fetch topic. While we could have set up an S3 trigger on the receiving buckets to do this work, we chose not to for a few reasons. First, during our scheduled maintenance windows, Pharos can be offline for two hours or more. In addition, during routine deployments, Pharos may be unavailable for a few seconds. In both cases, data from the S3 triggers would be lost, as there would be no endpoint to receive them. Second, to ensure our system is portable and not dependent on vendor-specific features, we wanted to avoid AWS-specific lambda triggers that may not port to open-source S3-compliant systems like Minio or even to other cloud providers like Wasabi. Third, the cron job provides a simple mechanism for throttling ingest during periods of heavy load. It's easier to change the timing on a single cron job than to suspend and then re-enable triggers in twenty or so S3 buckets.","title":"Bucket Reader"},{"location":"services/ingest/cleanup/","text":"Cleanup After a bag has been ingested, the cleanup worker does the following: deletes all of the interim data associated with the WorkItem from Redis deletes all of the bag's interim files from the staging bucket deletes the original tarred bag from the depositor's receiving bucket marks the WorkItem complete Resource Usage The cleanup worker uses few resources of any kind.","title":"Cleanup"},{"location":"services/ingest/cleanup/#cleanup","text":"After a bag has been ingested, the cleanup worker does the following: deletes all of the interim data associated with the WorkItem from Redis deletes all of the bag's interim files from the staging bucket deletes the original tarred bag from the depositor's receiving bucket marks the WorkItem complete","title":"Cleanup"},{"location":"services/ingest/cleanup/#resource-usage","text":"The cleanup worker uses few resources of any kind.","title":"Resource Usage"},{"location":"services/ingest/format_identification/","text":"Format Identification The format identification worker streams a chunk of each file in the staging bucket through the Siegfried format identifier, which compares file signatures to a PRONOM database. Because Siegfried is written in Go, it's embedded in the worker. If Siegfried can identify the file by signature, the worker stores the format information in the file's interim record in Redis. If Siegfried cannot identify the file by signature, the worker leaves the file's extension-based format identification (noted by the pre-fetch worker) unchanged. Resource Usage The format identification worker: can use substantial amounts of nework IO, primarily in reads from the S3 staging bucket. can use substantial CPU when identifying a large number of files (e.g. if a bag contains 10,000 files) makes one read and one update call to Pharos' WorkItems endpoint makes one read and one update call per file to Redis","title":"Format Identification"},{"location":"services/ingest/format_identification/#format-identification","text":"The format identification worker streams a chunk of each file in the staging bucket through the Siegfried format identifier, which compares file signatures to a PRONOM database. Because Siegfried is written in Go, it's embedded in the worker. If Siegfried can identify the file by signature, the worker stores the format information in the file's interim record in Redis. If Siegfried cannot identify the file by signature, the worker leaves the file's extension-based format identification (noted by the pre-fetch worker) unchanged.","title":"Format Identification"},{"location":"services/ingest/format_identification/#resource-usage","text":"The format identification worker: can use substantial amounts of nework IO, primarily in reads from the S3 staging bucket. can use substantial CPU when identifying a large number of files (e.g. if a bag contains 10,000 files) makes one read and one update call to Pharos' WorkItems endpoint makes one read and one update call per file to Redis","title":"Resource Usage"},{"location":"services/ingest/metadata_recording/","text":"Metadata Recording After storage validation is complete, the record worker records all information about the ingest in Pharos. It copies the interim object record and all of the interim file records from Redis to Pharos. It also creates all required PREMIS events for the ingest. For bags containing large numbers of files, this step can be very taxing on Pharos. Each ingest produces the following number of records: 1 intellectual object creation or update 3 object-related PREMIS events 1 generic file creation or update per file 6 PREMIS events per file 3 checksum creations per file A new bag containing 10,000 files would create 100,004 new records through Pharos (4 for the object and 10,000 * 10 for the files). While the file records are recorded in batches, the operation is still taxing. Like other workers, the recorder marks each object and file as saved once Pharos returns a successful response, and then saves each object/file back to Redis. If recording fails before completion, the next worker to pick up the task will know what's been saved and what hasn't. Resource Usage One read and one update for every interim file record in Redis. One read and one update of the Pharos WorkItem. Heavy POST and/or PUT operations to Pharos to record file, checksum, and event data. In general, we should not run more than one record worker to avoid overwhelming Pharos during heavy ingest. The single record worker can be tuned to use a set number of workers to utilize Pharos to its limits without overwhelming it.","title":"Metadata Recording"},{"location":"services/ingest/metadata_recording/#metadata-recording","text":"After storage validation is complete, the record worker records all information about the ingest in Pharos. It copies the interim object record and all of the interim file records from Redis to Pharos. It also creates all required PREMIS events for the ingest. For bags containing large numbers of files, this step can be very taxing on Pharos. Each ingest produces the following number of records: 1 intellectual object creation or update 3 object-related PREMIS events 1 generic file creation or update per file 6 PREMIS events per file 3 checksum creations per file A new bag containing 10,000 files would create 100,004 new records through Pharos (4 for the object and 10,000 * 10 for the files). While the file records are recorded in batches, the operation is still taxing. Like other workers, the recorder marks each object and file as saved once Pharos returns a successful response, and then saves each object/file back to Redis. If recording fails before completion, the next worker to pick up the task will know what's been saved and what hasn't.","title":"Metadata Recording"},{"location":"services/ingest/metadata_recording/#resource-usage","text":"One read and one update for every interim file record in Redis. One read and one update of the Pharos WorkItem. Heavy POST and/or PUT operations to Pharos to record file, checksum, and event data. In general, we should not run more than one record worker to avoid overwhelming Pharos during heavy ingest. The single record worker can be tuned to use a set number of workers to utilize Pharos to its limits without overwhelming it.","title":"Resource Usage"},{"location":"services/ingest/pre_fetch/","text":"Pre-Fetch The pre-fetch worker, ingest_pre_fetch , streams a tarred bag from a receiving bucket through a tar reader to extract metadata. All of the extracted metadata is stored in Redis, where subsequent workers can access it. The pre-fetch worker collects the following data as it reads the bag: the URL of the bag's BagIt profile (if not found, defaults to APTrust) file names file sizes a first guess at the file's format, based on extension checksums read from manifests (and or all of md5, sha1, sha256 and sha512) checksums calculated from files (md5, sha1, sha256, sha512) tag names and values parsed from bagit.txt, bag-info.txt, aptrust-info.txt, and any other text-based non-payload files that use BagIt tag file format The pre-fetch worker requeues items with transient errors. When it encounters fatal errors, it marks the WorkItem as Failed. Transient errors are almost always network errors, such as \"connection reset by peer.\" Fatal errors include invalid file format (i.e. the bag is not really a tar file) and \"key not found,\" which means the bag was deleted from the recieving bucket before the worker could get to it. The pre-fetch will mark the WorkItem as cancelled and not process it if the etag of the bag in the receiving bucket does not match the etag in the WorkItem. This happens when the depositor uploads a new version of the bag, overwriting the version to which the original WorkItem referred. For information about how the pre-fetch worker stores information, see Redis . Resource Usage The pre-fetch worker: Uses a lot of bandwidth downloading large bags or large quantities of small bags from S3 receiving buckets. It does not write the data to disk, but it does pull it across the network. Uses a noticeable amount of CPU while calculating checksums. Makes many calls to Redis, mostly writes, to save JSON data about the object and each file in the bag. Makes a few calls to Pharos, mainly to read and update a WorkItem, but generally does not tax Pharos.","title":"Pre-Fetch"},{"location":"services/ingest/pre_fetch/#pre-fetch","text":"The pre-fetch worker, ingest_pre_fetch , streams a tarred bag from a receiving bucket through a tar reader to extract metadata. All of the extracted metadata is stored in Redis, where subsequent workers can access it. The pre-fetch worker collects the following data as it reads the bag: the URL of the bag's BagIt profile (if not found, defaults to APTrust) file names file sizes a first guess at the file's format, based on extension checksums read from manifests (and or all of md5, sha1, sha256 and sha512) checksums calculated from files (md5, sha1, sha256, sha512) tag names and values parsed from bagit.txt, bag-info.txt, aptrust-info.txt, and any other text-based non-payload files that use BagIt tag file format The pre-fetch worker requeues items with transient errors. When it encounters fatal errors, it marks the WorkItem as Failed. Transient errors are almost always network errors, such as \"connection reset by peer.\" Fatal errors include invalid file format (i.e. the bag is not really a tar file) and \"key not found,\" which means the bag was deleted from the recieving bucket before the worker could get to it. The pre-fetch will mark the WorkItem as cancelled and not process it if the etag of the bag in the receiving bucket does not match the etag in the WorkItem. This happens when the depositor uploads a new version of the bag, overwriting the version to which the original WorkItem referred. For information about how the pre-fetch worker stores information, see Redis .","title":"Pre-Fetch"},{"location":"services/ingest/pre_fetch/#resource-usage","text":"The pre-fetch worker: Uses a lot of bandwidth downloading large bags or large quantities of small bags from S3 receiving buckets. It does not write the data to disk, but it does pull it across the network. Uses a noticeable amount of CPU while calculating checksums. Makes many calls to Redis, mostly writes, to save JSON data about the object and each file in the bag. Makes a few calls to Pharos, mainly to read and update a WorkItem, but generally does not tax Pharos.","title":"Resource Usage"},{"location":"services/ingest/reingest_check/","text":"Reingest Check The reingest worker checks Pharos to see if the bag being ingested has ever been ingested before. If Pharos has a record for a bag with the same name, belonging to the same institution, and the bag record is not marked deleted (state == 'D'), then we're reingesting an existing bag. In this case, the reingest worker checks to see if Pharos has a non-deleted record for each file in the bag being reingested. If so, the reingest worker: Assigns the UUID of the existing Pharos file to the interim IngestFile record in Redis. We do this because, if we're going to move the new file into preservation storage, we want it to overwrite the old one. To do that, it must have the same UUID (S3 key) as the existing file. We also do not want to store multiple copies of any file under multiple UUIDs in preservation storage, because we have no way of tracking more than one perservation UUID per file. Checks whether the checksum of the new file matches the checksum of the existing file as reported by Pharos. If the checksums do not match, the new file is marked as needing to be copied to preservation. If the checksum has not changed, the file is marked as not needing to be copied. Forces the Storage Option of all files to match the Storage-Option of existing files in Pharos so that we do not wind up with mismatched versions of files in different S3 buckets, Glacier vaults and Wasabi buckets. The rationale for this is documented in our user guide and has been explained verbally to depositors at our tech and in-person meetings. The reingest worker updates object and file records in Redis to record the results of its work. Resource Usage For new bags, the reingest worker uses few resources. It reads and updates a Pharos WorkItem, makes one query to the Pharos object endpoint, and reads and updates one object record in Redis. For reingests, it does all of the above plus the following: makes two queries to Pharos for each file in the bag: one for the GenericFile record, and one for the file's checksums makes one read request and one write request to Redis for each file Since most bags contain between 5 and 20 files, this typically isn't too bad. Some bags, however, contain hundreds of thousands of files. In those more extreme cases, this worker may put a heavy load on Pharos and Redis.","title":"Reingest Check"},{"location":"services/ingest/reingest_check/#reingest-check","text":"The reingest worker checks Pharos to see if the bag being ingested has ever been ingested before. If Pharos has a record for a bag with the same name, belonging to the same institution, and the bag record is not marked deleted (state == 'D'), then we're reingesting an existing bag. In this case, the reingest worker checks to see if Pharos has a non-deleted record for each file in the bag being reingested. If so, the reingest worker: Assigns the UUID of the existing Pharos file to the interim IngestFile record in Redis. We do this because, if we're going to move the new file into preservation storage, we want it to overwrite the old one. To do that, it must have the same UUID (S3 key) as the existing file. We also do not want to store multiple copies of any file under multiple UUIDs in preservation storage, because we have no way of tracking more than one perservation UUID per file. Checks whether the checksum of the new file matches the checksum of the existing file as reported by Pharos. If the checksums do not match, the new file is marked as needing to be copied to preservation. If the checksum has not changed, the file is marked as not needing to be copied. Forces the Storage Option of all files to match the Storage-Option of existing files in Pharos so that we do not wind up with mismatched versions of files in different S3 buckets, Glacier vaults and Wasabi buckets. The rationale for this is documented in our user guide and has been explained verbally to depositors at our tech and in-person meetings. The reingest worker updates object and file records in Redis to record the results of its work.","title":"Reingest Check"},{"location":"services/ingest/reingest_check/#resource-usage","text":"For new bags, the reingest worker uses few resources. It reads and updates a Pharos WorkItem, makes one query to the Pharos object endpoint, and reads and updates one object record in Redis. For reingests, it does all of the above plus the following: makes two queries to Pharos for each file in the bag: one for the GenericFile record, and one for the file's checksums makes one read request and one write request to Redis for each file Since most bags contain between 5 and 20 files, this typically isn't too bad. Some bags, however, contain hundreds of thousands of files. In those more extreme cases, this worker may put a heavy load on Pharos and Redis.","title":"Resource Usage"},{"location":"services/ingest/staging/","text":"Copy to Staging After the reingest check, the ingest_staging_uploader does the following: Streams the tarred bag from the depositor's receiving bucket through a tar reader. Copies files from the tar reader, one by one, into the staging bucket. Because tar is a sequential format, the tar reader must read and upload files one at a time. No data ever touches the local disk in this operation. The staging uploader streams tarred data down from one bucket and sends untarred data back to another. The efficiency and cost effectiveness of this process depends on the receiving buckets, the staging bucket, and the staging uploader all running within the same AWS region. We copy files to a staging bucket instead of to a local disk for two reasons: We never know how much disk space we will need at any given time. In practice, we know it varies from less than 1 GB to over 20 TB. In practice, scaling EBS volumes was problematic and expensive and EFS just didn't work. You'll find more on those issues in our white paper on why we chose S3 for staging We want the unpacked files to be available to all workers on any EC2 instance for further processing. Storing them on a local EBS volume makes them accessible only to the server to which the volume is attached. Resource Usage The staging uploader uses large amounts of bandwidth for simultaneous sending and receiving. It also uses a noticeable amount of CPU, because S3 copies calculate checksums/etags as they go, and memory, because both the uploader and downloader buffer contents as they read/write.","title":"Copy to Staging"},{"location":"services/ingest/staging/#copy-to-staging","text":"After the reingest check, the ingest_staging_uploader does the following: Streams the tarred bag from the depositor's receiving bucket through a tar reader. Copies files from the tar reader, one by one, into the staging bucket. Because tar is a sequential format, the tar reader must read and upload files one at a time. No data ever touches the local disk in this operation. The staging uploader streams tarred data down from one bucket and sends untarred data back to another. The efficiency and cost effectiveness of this process depends on the receiving buckets, the staging bucket, and the staging uploader all running within the same AWS region. We copy files to a staging bucket instead of to a local disk for two reasons: We never know how much disk space we will need at any given time. In practice, we know it varies from less than 1 GB to over 20 TB. In practice, scaling EBS volumes was problematic and expensive and EFS just didn't work. You'll find more on those issues in our white paper on why we chose S3 for staging We want the unpacked files to be available to all workers on any EC2 instance for further processing. Storing them on a local EBS volume makes them accessible only to the server to which the volume is attached.","title":"Copy to Staging"},{"location":"services/ingest/staging/#resource-usage","text":"The staging uploader uses large amounts of bandwidth for simultaneous sending and receiving. It also uses a noticeable amount of CPU, because S3 copies calculate checksums/etags as they go, and memory, because both the uploader and downloader buffer contents as they read/write.","title":"Resource Usage"},{"location":"services/ingest/storage/","text":"Copy to Preservation The apt_preservation_uploader worker copies files from the staging bucket to preservation buckets. For new bags, it copies all files. For reingests, it copies only those files that are new or have been changed since the last ingest. This worker uses server-side copying (direct S3-to-S3 copies that use none of our EC2 instance's bandwidth) when copying from staging to any other AWS bucket in the same region (US East 1). For all other uploads, it streams each file down from the staging bucket and then back out to the preservation bucket. The bits never touch local disk. Although we could use server-side copying for all AWS-hosted preservation buckets, we've chosen not to because AWS throttles cross-region S3-to-S3 copies at 50 MiB/second, with performance often below that. Streaming the data through the EC2 instance is typically six times faster (or more). This makes a big difference on large files, which take hours instead of days to copy. In addition to copying file contents into preservation storage, the uploader also copies the following metadata: Content-Type - The file's mime type, as identified by Siegfried or extension. x-amz-meta-institution - The identifier of the institution that owns the bag/intellectual object. E.g. virginia.edu, emory.edu, etc. x-amz-meta-bag - The name of the bag to which the file belongs. E.g. virginia.edu/bag-of-photos. x-amz-meta-bagpath - The path this file occupied within the bag. x-amz-meta-md5 - The file's md5 digest, as caculated at ingest. x-amz-meta-sha256 - The file's sha256 digest, as calculated at ingest. Resource Usage The preservation uploader: uses a lot of bandwidth for both downloading from staging and uploading to preservation. uses a noticeable amount of CPU (for on-the-fly etag calculation) and memory (for read/write buffers). makes one read and one write call to Redis for each file makes one read one update call to Pharos for the WorkItem","title":"Copy to Preservation"},{"location":"services/ingest/storage/#copy-to-preservation","text":"The apt_preservation_uploader worker copies files from the staging bucket to preservation buckets. For new bags, it copies all files. For reingests, it copies only those files that are new or have been changed since the last ingest. This worker uses server-side copying (direct S3-to-S3 copies that use none of our EC2 instance's bandwidth) when copying from staging to any other AWS bucket in the same region (US East 1). For all other uploads, it streams each file down from the staging bucket and then back out to the preservation bucket. The bits never touch local disk. Although we could use server-side copying for all AWS-hosted preservation buckets, we've chosen not to because AWS throttles cross-region S3-to-S3 copies at 50 MiB/second, with performance often below that. Streaming the data through the EC2 instance is typically six times faster (or more). This makes a big difference on large files, which take hours instead of days to copy. In addition to copying file contents into preservation storage, the uploader also copies the following metadata: Content-Type - The file's mime type, as identified by Siegfried or extension. x-amz-meta-institution - The identifier of the institution that owns the bag/intellectual object. E.g. virginia.edu, emory.edu, etc. x-amz-meta-bag - The name of the bag to which the file belongs. E.g. virginia.edu/bag-of-photos. x-amz-meta-bagpath - The path this file occupied within the bag. x-amz-meta-md5 - The file's md5 digest, as caculated at ingest. x-amz-meta-sha256 - The file's sha256 digest, as calculated at ingest.","title":"Copy to Preservation"},{"location":"services/ingest/storage/#resource-usage","text":"The preservation uploader: uses a lot of bandwidth for both downloading from staging and uploading to preservation. uses a noticeable amount of CPU (for on-the-fly etag calculation) and memory (for read/write buffers). makes one read and one write call to Redis for each file makes one read one update call to Pharos for the WorkItem","title":"Resource Usage"},{"location":"services/ingest/storage_validation/","text":"Storage Validation After the preservation uploader copies files to preservation storage, the verifier queries the S3 bucket to ensure that each file is present and that the size of each matches the size recorded in the Redis interim data. We don't currently check for etag matches, because etags can change, depending on how files are chunked in multipart uploads. We include this step because of a peristent bug in our second-generation services in which AWS S3 servers reported that files were copied even when zero bytes were written. The verified marks each file record as verified and saves the information back to Redis. If any file fails verification, the verifier will mark the WorkItem as failed, and it will be up to the APTrust administrator to manually requeue the item (through the Pharos UI) in the ingest06_storage topic. The requeue process here is not automated, because we want the APTrust admin to look into the issue. (As of August, 2020, storage validation has not failed on the 100,000+ files we've pushed through our staging system.) Resource Usage This worker issues a potentially large number of stat/head requests to S3, sending and receiving only a kilobyte or so of information in each request. It uses very little CPU or memory.","title":"Storage Validation"},{"location":"services/ingest/storage_validation/#storage-validation","text":"After the preservation uploader copies files to preservation storage, the verifier queries the S3 bucket to ensure that each file is present and that the size of each matches the size recorded in the Redis interim data. We don't currently check for etag matches, because etags can change, depending on how files are chunked in multipart uploads. We include this step because of a peristent bug in our second-generation services in which AWS S3 servers reported that files were copied even when zero bytes were written. The verified marks each file record as verified and saves the information back to Redis. If any file fails verification, the verifier will mark the WorkItem as failed, and it will be up to the APTrust administrator to manually requeue the item (through the Pharos UI) in the ingest06_storage topic. The requeue process here is not automated, because we want the APTrust admin to look into the issue. (As of August, 2020, storage validation has not failed on the 100,000+ files we've pushed through our staging system.)","title":"Storage Validation"},{"location":"services/ingest/storage_validation/#resource-usage","text":"This worker issues a potentially large number of stat/head requests to S3, sending and receiving only a kilobyte or so of information in each request. It uses very little CPU or memory.","title":"Resource Usage"},{"location":"services/ingest/validation/","text":"Validation The validation worker validates the metadata that the pre-fetch worker stored in Redis. It checks that: The bag has all tag files required by the BagIt profile. The bag has all required tags with valid values, as defined by the BagIt profile.. All files listed in the manifests and tag manifests are present. All file checksums match checksums in the manifests. There are no extraneous files in payload directory. If the bag passes validation, the worker updates the WorkItem in Pharos and pushes the WorkItem ID into the next NSQ topic, which is the reingest check topic. If there are transient errors, which are almost always network errors, the worker requeues the item and notes the transient errors in the Pharos WorkItem. If the bag is invalid, that's a fatal error. The worker marks the WorkItem as failed, including a description of why it failed validation, then it pushes the item to the Cleanup queue. Resource Usage The validation worker normally completes quickly without taxing any systems or resources too heavily. It: Makes many read calls to Redis, including one for the object and one for each of its files. Reads and updates the WorkItem in Pharos.","title":"Validation"},{"location":"services/ingest/validation/#validation","text":"The validation worker validates the metadata that the pre-fetch worker stored in Redis. It checks that: The bag has all tag files required by the BagIt profile. The bag has all required tags with valid values, as defined by the BagIt profile.. All files listed in the manifests and tag manifests are present. All file checksums match checksums in the manifests. There are no extraneous files in payload directory. If the bag passes validation, the worker updates the WorkItem in Pharos and pushes the WorkItem ID into the next NSQ topic, which is the reingest check topic. If there are transient errors, which are almost always network errors, the worker requeues the item and notes the transient errors in the Pharos WorkItem. If the bag is invalid, that's a fatal error. The worker marks the WorkItem as failed, including a description of why it failed validation, then it pushes the item to the Cleanup queue.","title":"Validation"},{"location":"services/ingest/validation/#resource-usage","text":"The validation worker normally completes quickly without taxing any systems or resources too heavily. It: Makes many read calls to Redis, including one for the object and one for each of its files. Reads and updates the WorkItem in Pharos.","title":"Resource Usage"}]}